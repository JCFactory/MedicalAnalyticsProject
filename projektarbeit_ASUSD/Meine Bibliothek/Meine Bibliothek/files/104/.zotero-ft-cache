
Loading [MathJax]/extensions/MathEvents.js
NIPS Proceedings β

    Books
    2007

Spatial Latent Dirichlet Allocation

Part of: Advances in Neural Information Processing Systems 20 (NIPS 2007)
[PDF] [BibTeX] [Supplemental]
Authors

    Xiaogang Wang
    Eric Grimson

Abstract

In recent years, the language model Latent Dirichlet Allocation (LDA), which clusters co-occurring words into topics, has been widely appled in the computer vision field. However, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since LDA assumes that a document is a bag-of-words''. It is also critical to properly design words'' and “documents” when using a language model to solve vision problems. In this paper, we propose a topic model Spatial Latent Dirichlet Allocation (SLDA), which better encodes spatial structure among visual words that are essential for solving many vision problems. The spatial information is not encoded in the value of visual words but in the design of documents. Instead of knowing the partition of words into documents \textit{a priori}, the word-document assignment becomes a random hidden variable in SLDA. There is a generative procedure, where knowledge of spatial structure can be flexibly added as a prior, grouping visual words which are close in space into the same document. We use SLDA to discover objects from a collection of images, and show it achieves better performance than LDA.
Neural Information Processing Systems (NIPS)

Papers published at the Neural Information Processing Systems Conference.
© 1987 – 2019 Neural Information Processing Systems Foundation, Inc.
() {} []
