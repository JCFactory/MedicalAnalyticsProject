\input{preamble}
\input{Abkuerzung}

\begin{document}
\input{Titelseite}

\chapter{Abstract}\label{abstract}
\ref{abstract}

\chapter{Introduction}\label{introduction}

\gls{lda} provides many advantages, such as that it is suitable for large data and that it performs very well in extracting topics for Indonesian text documents \footnote{\autocite{twinandilla_2018}}.

\chapter{Related work}\label{related}

Zhao et al. \footnote{\autocite{zhao_2016}} describe how topic modeling can be used to analyze \gls{ngs}. By implementing topic modelling, text corpus are generated \footnote{\autocite{zhao_2016}}.

In the beginning of every genome analysis, there are several important questions to ask. Jurca et al. \footnote{\autocite{jurca_2016}} recommend to ask the following questions: What are the top studied genes in breast cancer? How regulated blood cancer research is in each country? Which countries have studied the largest number of breast cancer? Which are the popular genes mentioned together by countries every year? Where do key genes lie in the soft clusters?

Jurca et al. describe a process to use large-scale text analysis of biomedical abstracts in order to generate new hypothesis about cancer biomarkers \footnote{\autocite{jurca_2016}}. The target is to develop a data minng methodology that patterns in genes associated with cancer. By analyzing disease-specific gene expression data, experimental data is being checked whether a gene has indeed been upregulated or downregulated with respect to a disease.

According to Xu et al. \footnote{\autocite{xu_2013}}, \gls{mirna}s build a class of 17-27 nucleotides single-stranded \gls{rna} molecules that regulate gen expression post-transcriptionally. In the described text-mining process, Xu et al. identified nine \gls{mirna}s in bladder cancer and adopted protein-protein interaction sites between these miRNAs and target genes. The results of the analyzation process lead to two relationship types between bladder cancer and its \gls{mirna}: casual and unspecified. 

Topic modelling is not only used to analyze relationships between genomes but also to improve diagnoses for stroke disease. Djatna et al. \footnote{\autocite{djatna_2018}} describe an 'Intuitionistic Fuzzy Based Decision Tree' to diagnose different types of stroke disease. To be precise, the different types of stroke diseases can be calculated by Hamming distance. The term 'Fuzzy logic' means logic that underlies the reasoning of data by way of precise estimates. It is the fastest way to map input space into output space using a degree of membership.

Lloret et al. \footnote{\autocite{lloret_2012}} built an automatic summarization algorithm for literature. It can includes three steps: First, topic identification, second topic interpretation and third summary generation. While describing the process of textual analyzation, Lloret et al. mention a specific term: \gls{tf-idf} which is important for topic modelling. In addition to topic-based approaches, there are graph-based approaches and discourse-based approaches. Graph-based approaches implicate nodes that represent text elements and the edges/links refer to synonymy \footnote{\autocite{lloret_2012}}. Discourse-based approaches include \gls{rst}, \gls{hmm}, \gls{rst} or \gls{bm}. 

Yang et al. \footnote{\autocite{yang_2018}} describe a process of 'constructing a database for relations between human \gls{cnv}s and human genetic disease via systematic text mining'. In general, \gls{cnv} can cause disease by gene dosage, disruption, fusion or other genetic position effects. 
To be more precise, there can \gls{cnv} can lead to two types of autosomal variants: They can either cause deletion or amplification of the long or broken arm region of chromosomes 1-22 or can build multiples of chromosomes 1-21 (e.g. as in disease trisomy 21). 

According to their article, Yang et al. used \gls{cnv} database which linked the \gls{cnv} information to the \gls{ncbi} Gene and Ontology database. In their article, Yang et al.\footnote{\autocite{yang_2018}} mention three steps in the text mining process. First, during the pre-processing step, unstructured fields are split into separated sentences by using \gls{nltk}, a python package \footnote{\autocite{nltk}}. After that, in the \gls{ner} step, all disease mentions within DNorm system, such as MeSH IDs are recognized. In the third step, \gls{re}, the positions in sentences and entities are compared to generate instances that constist of two candidate entities within one single sentence.

Yang et al. mention two more processing methods: Parallel Processing and Post Processing which includes data cleaning and statistics \footnote{\autocite{yang_2018}}. The term 'data cleaning' is explained as 'de-duplicating data after each step of the process to reduce repetitive operations and prevent statistical errors'. This is a very useful step in biomedicine since as a common problem, biomedical databases contain errors. For that reason, users can give feedback through a feedback mechanism to improve the quality of the databases.

Lu et al. \footnote{\autocite{lu_2016}} used multi-channel \gls{lda} to model healthcare data. In fact, by creating a learned latent variable model, the likelihood of a set of diagnosis, medications, contextual information in a patient's record can be evaluated. This can help to identify outliers and improve medical data quality. Furthermore, disease groups can be identified. Missing medication or diagnosis can be predicted. Lu et al. used \gls{arm} and supervised learning to predict missing medications. The topic model defines how words in a document are generated through the control of latent topics.

\chapter{\gls{lda}}\label{lda}
\section{General description}\label{lda_description}
According to Jurca et al. \footnote{\autocite{jurca_2016}} the text mining process can be divided into four steps: First, the information has to be retrieved by user queries (\gls{ir}). Second, different vocabularies and ontologies have to be integrated (\gls{ner}). Third, during \gls{ie}, relationships between biological entities in the texts are extracted by either use co-occurence processing or \gls{nlp}. Last, there has to be gained biologically meaningful knowledge about how biological entities are related by using \gls{kd} methods.

Moreover, there can be distinguished between three types of clustering: hard clustering, hierarchical clustering and soft clusternig. Hard clustering describes the process of separating items into distinct groups where each item is exactly in one cluster. Hierarchical clustering implicates single-link (how similar the items are to one another) and complete-link (how dissimilar the items are). Soft clustering means that items cannot be distinctly separated into clusters and partly are member of two or more clusters at a time \footnote{\autocite{jurca_2016}}.  

Besides, Djatna et al. \footnote{\autocite{djatna_2018}} mention data mining techniques, such as \gls{cart}, \gls{id3}, \gls{dt} and two classification techniques: \gls{pca} and \gls{lda}. 

\gls{lda} was developed by David Blei et. al in the year 2003 and is a clustering algorithm for text mining. It counts to the most popular topic modelling algorithms \footnote{\autocite{zhao_2016}}.
According to \footnote{\autocite{zhao_2016}}, topic modelling requires of a number documents which represent each of them a mixture of latent topics. Moreover, each topic is expressed by a distribution of words. During \gls{lda}, two relationships are analyzed: First, the relationship between documents and words, also called 'per-document topic distributions'. Second, the relationship between words and topics ('per-topic word distributions'). To measure the relationships exactly and to make inference about topics and documents for text mining, probability matrices are calculated.

Lu et al. \footnote{\autocite{lu_2016}} define topic models as a text mining approach that assumes observed word co-occurences which are governed by latent variables. \gls{lda} includes the identification of latent topics from a set of documents, analyzing long-term topic trends and modelling words and references in documents.   

\gls{lda} is a probabilistic (Bayesian) model of text documents \footnote{\autocite{hoffman_2010}}. The idea of \gls{lda} is to define a document as a collection of k topics. Each topic definces a multinominal distribution over a vocabulary which is drawn from a dirichlet.

Park et al. \footnote{\autocite{park_2009}} define topic models as follows: Documents are no longer a collection of words, but a collection of topics. Furthermore, \gls{lda} is a generative topic model which uses a dirichlet parameter (also called dirichlet prior) to model documents. By changing the dirichlet prior, the number of topics that the model assigns to each word and document can be controlled. To be more precisely, a small dirichlet prior means a small number of topics assigned to each word. By increasing the dirichlet prior, the distribution of topics to each word rises. Moreover, the dirichlet parameter is obtained for each document and can be fitted using a maximum or estimated likelihood. If the dirichlet parameter does not fit, the gain in computational efficiency is obtained. Otherwise, there is no advantage (when dirichlet parameter fits well). 

What is more, every term has a probabilistic relationship to every document. The topic model probabilities are stored as term relationships in thesaurus. The term frequencies are stored in the document index.

Twinadilla et al. \footnote{\autocite{twinandilla_2018}} mention three variables to be defined before the \gls{lda} process: $\alpha$ (the diversity of sentence distribution), $\beta$ (the diversity of topic distribution) and $\gamma$ (the similarity between sentences and titles).

In their article 'Multi-document summarization using k-means and \gls{lda}-significant sentences, Twinandilla et al. \footnote{\autocite{twinandilla_2018}} describe the research process by implementing the following six steps.

\begin{description}
\item[1.step: Preprocessing]\hfill \\
First, all words and sentences need to be simplified by using a bag of words as well a bag of sentences. In detail, this step includes case folding (putting all words into lower case), tokenization (cutting a document into an array of words or sentences and eleminating punctuation), stopword removal (deleting words that appear often without a particular meaning) and stemming (changing words in a document that appear often wothout a particular meaning).

\item[2.step: Calculate the number of clusters]\hfill \\
Second, the number of clusters needs to be calculated by using k-means clustering. 

\item[3.step: \gls{lda}]\hfill \\
In this step, Twinandilla et al. distinguish between generative and inference \gls{lda}. Generative \gls{lda} forms a document from a collection of words whereas inference \gls{lda} only retrieves information from documents.

\item[4.step: Sentence \gls{lda}]\hfill \\
Fourth, during sentence \gls{lda}, documents are represented as topic representation. Each topic is a sentence distribution that represents a sentence. This sentence has significant weight on a multi-document summarization.

\item[5.step: Summary formation]\hfill \\
In this step, each document is sorted by a decreasing value of the final sentence weight. After that, the p percent of sentences with the highest value has to be chosen from each document. The p value is subsequently called 'summarization level'.

\item[6.step: Arrange selected sentences in a sequence]\hfill \\
The last step includes the arrangement of all selected sentences in a sequence. This means putting them into a useful order to summarize all documents.
\end{description}

As reported by Blei et al. \footnote{\autocite{blei_2003}}, \gls{lda} is a generative probabilistic model for collections of discrete data such as text corpora. In addition to that, \gls{lda} is represented as three level hierarchical Bayesian model in which each item of a collection is a finite mixture over an underlying set of topics. What is more, each topic is modeled as an infinite mixture ofver an underlying set of probabilities. Blei et al. \footnote{\autocite{blei_2003}} define topic probabilities as an explicit representation of a document.

\paragraph{\gls{tf-idf}}

\gls{tf-idf} is a scheme through which a basic vocabulary of words or terms is chosen. For each document in the corpus a count (which represents the number of occurences for each word) is formed \footnote{\autocite{blei_2003}}. 
There are three terms to be distinguished: First, a word is a basic unit of descrete data, defined to be an item from the vocabulary indexed by {1 ... V}. Second, a document is a sequence of N words. Third, a corpus is a collection of M documents.
After a suitable normalization process, the term frequency count is compared to an inverse document frequency count. This leads to the total number of occurences of a word in the entire corpus.
The result is a term-by-document matrix X whose columns contain \gls{tf-idf} valuesfor each document in the corpus. 

\paragraph{Process of \gls{lda}}

The process of \gls{lda} can be briefly described with the following steps \footnote{\autocite{wang_2008}}: First, there are M documents in the corpus. Second, each document j has \(N_{j}\) words. In the next step, the observed value \(w_{ji}\) describes the appearance of a word i in a document j. In addition to that, all words will be clustered into K topics which are defined as object classes. Finally, each topic k is modelled as a multinomial distribution over the codebook.

As reported by Wang et al. \footnote{\autocite{wang_2008}}, \gls{lda} is a language model which clusters co-occuring words into topics. Moreover, documents are described as 'bag of words'. In order to analyze documents, users need to define the meaning of documents in vision problems. 
Wang et al. \footnote{\autocite{wang_2008}} describe a special form of \gls{lda}: Spatial \gls{lda}. It encodes spatial structure among visual words, assuming the partition of words into documents is known a priori. 

Extension of \gls{lda} are author-topic model, dynamic-topic model and correlated topic model. Wang et al. refer to the dynamic-topic model while describing how visual words are clustered into topics which correspond to object classes.
 
\section{Examples and possible use cases}\label{lda_examples}

Zhao et al. describe the process of analyzing genomes as follows: First, each document corresponds to one of the total number of \gls{dna} straints. Second, all documents had the same number of words. Third, the distribution of words for topics as well as the distribution of topics in documents were described by random variables obeying Dirichlet distributions with parameters $\alpha$ and $\beta$. After that, nucleotides and their orders in \gls{ngs} sequences coudl be treated as words and the genetic information in sequences was translated and exhibited as a 'bag of words'  \footnote{\autocite{zhao_2016}}. By using the strain-topic matrix derived from topic modelling, relationships or similarities between the strains serotypes can be found out. 

Hoffman et al. \footnote{\autocite{hoffman_2010}} describe the development of an online variational Bayes algorithm for \gls{lda} which is based on stochastic optimization with a natural gradient step. This step converges to a local optimum of the variational Bayes objective function.
To be more precise, Bayesian models provide a natural way to encode assumptions about observed data.
There can be distinguished between two approaches: First, sampling approaches are based on \gls{mcmc} sampling. Here, a Markov chain defines the stationary distribution.
Second, there are optimization approaches which are usually based on variational inference \gls{mcmc}. In this case, variational Bayes optimizes the simplified parametric distribution.

Twinandilla et al. \footnote{\autocite{twinandilla_2018}} developed a 'multi-document summarization using k-means and \gls{lda}-significant sentences' on yellow journalism. Twinadilla explains this kind of journalism as 'redundant news documents' which makes it difficult to distinguish documents containing fact or opinionated information. After defining the corpus, Twinandilla et al. describe two different summarization processes: Abstractive summarization means summarizing documents by creating new sentences (with the same information as the original document). Extractive summarization suggests summarizing a document by selecting a part of a sentence in that document.

As described in \ref{related} on page \pageref{related}, Lu et al. \footnote{\autocite{lu_2016}} used \gls{mclda} to estimate latent health status groups. \gls{mclda} constructs latent relations among diagnoses, medications, contextual variables in different status groups. To be more precise, it is a dimensional reduction method of that summarizes each record using a probability vector over a latent health status group. The prediction tasks were performed by using a \gls{cgs}-based  inference model and inferred methods. 
During the topic modelling process, Lu et al. refer to two associations among the data: diagnosis-medication associations to identify the clinical use of medications and diagnosis-diagnosis associations to create a network structure among the diseases.
 
\section{Python package 'Gensim'}\label{gensim}
              
\chapter{Acute Lymphoblastic Leukemia}\label{all}
\section{Types of Leukemia and its causes}\label{leukemia_types}
According to Jurca et al. \footnote{\autocite{jurca_2016}}, cancer is the result of damage, especially of mutations to cell's \gls{dna} which leads to a cell losing its normal functionality and gains the ability to indefinitely multiply until normal tissue funtions are impaired. This is also why malitious cancer is distributing so fast. Besides, each patient develops a different set of cancerous mutations in various genes which lead to multiple subtypes of cancer.
Furthermore, some genes can be up-regulated (which means that they are transcribed more and are expressed), down-regulated (which means that they are not expressed) or can be co-expressed (which means that they are expressed at the same time \footnote{\autocite{jurca_2016}}.

As stated by Montano et al. \footnote{\autocite{montano_2018}}, \gls{all} is a malignant disorder originating from hematopoietic B-/T-cell precursors which are characterized by marked heterogenity at molecular and clinical levels. There are many approaches to analyze these precursors, such as analyzing targeting of transcriptional factors (PAX5) which are involved in the pathogenesis of B-ALL. Other therapeutic and clinical approaches are genome editing techniques, i.e. the design of new therapies (\gls{car}s) and the study of genes involved in the evolution of pathogenesis.

\section{Examples for Genome Analysis: \gls{ngs}}\label{genome_analysis}
\gls{ngs} refers to post-Sanger sequencing methods \footnote{\autocite{zhao_2016}}. Since \gls{ngs} produces large volumes of sequence data it might be very useful to use topic modelling techniques to maintain the flexibility for the level of resolution required for given experiments.  
According to Gasperskaja et al. \footnote{\autocite{gasperskaja_2017}}, \gls{ngs} does not require a priori knowledge about genomic feature, it only requires a low amount of \gls{dna} or \gls{rna} as input.

The step before analyzing two or more (multiple) genomes is called alignment which includes a comparison of two genomes. There are many different types of alignments, but Zhao et al. refer to the \gls{msa} by describing \gls{muscle} and CLUSTAL.

Gasperskaja et al. \footnote{\autocite{gasperskaja_2017}} mention an important question which should be asked before every genome analysis: 'Is the variance pathogenic?' and whether there is any relationship between genotype and phenotype which means that it can lead to disease or can cause a number of disorders. 
Moreover, there can be distinguished between beneficial (\gls{snp}) and pathogenic (nonsense variant) single nucleotide changes, large microscopically visible or chromosomal aberation. 
To find out whether a genome mutation is pathogenic, Gasperskaja et al. \footnote{\autocite{gasperskaja_2017}} explain that substantial information about functional genomics can be found through analysis of \gls{mrna} or \gls{crna} (which is a copy from \gls{mrna} by reverse transcription \gls{pcr}.
Methods to measure \gls{rna} expression are for example: \gls{sage} or \gls{qpcr}.
By using \gls{cdna} microarray assays important genome-wide information about changes of gene expression in various cell lines can be found out.

As claimed by Montano et al. \footnote{\autocite{montano_2018}}, the development of \gls{ngs} techniques implicates vast amount of data which need to be translated. One important question is to find out how the genotype (the genetic expression of a biological attribute)influences the phenotype. By integrating genome editing systems into their research process, investigators are able to manipulate virtually any gene in a diverse range of cell types and organisms.
To give an example, Gasperskaja et al. \footnote{\autocite{gasperskaja_2017}} and Montano et al. \footnote{\autocite{montano_2018}} describe \gls{crispr-cas9}. In fact, this genome analysis includes generating a direct cut in the double strand of \gls{dna} by Cas9 nuclease. Cas9 is driven by a single 20-nucleotide \gls{rna} strand which marks the direct breakpoint. After cutting the \gls{dna}, the repair machinery of the host cell repairs errors and promotes a modification of the original sequence by a mutation (e.g. insertion, deletion, inversion).
But why should be used such a complicate process or why should we change the shape of \gls{dna}?
In the opinion of Montano et al. \footnote{\autocite{montano_2018}}, the use of genetically modified cell lines and animal models help us to better understand the functions of genes and their pathogenesis in diseases, such as cancer.
   
\section{Data sources: \gls{ncbi} and Ensembl genome browser 96}\label{datasources}

\chapter{Development of a solution for genetic analysis of \gls{all} genomes by implementing \gls{lda}}\label{development}
\section{Problems and challenges of genetic analysis}\label{problems_challenges}
Quality of data 
Is the data complete, which includes that it contains all required genomes which can cause \gls{all}.

our assumptions can lead to false content or solutions 

\section{First steps: Draft of developed solution}\label{draft}

To get useful data, the \gls{ncbi} \footnote{\autocite{ncbi}} was used to get all currently detected mutations of genomes which may cause \gls{lda}.

The first idea was to build a parsing application, which iterates over the found 582 genomes. After the iteration, it compares the oncogenes with the healthy genomes and to figure out where the differences are. The results might be displayed in a diagram. It might be possible to create clusters from the differences between the two groups or practice \gls{lda} on the differences.

\section{Proposed solution}\label{proposed_solution}
\section{Results}\label{results}


\chapter{Conclusion and Outlook}\label{conclusion_outlook}
\section{Lessons learned}\label{lessons_learned}
\section{Conclusion}\label{conclusion}
\section{Outlook}\label{outlook}	
	
% Beispiel f√ºr Bild
		\begin{figure}[htbp]
			\centering
			%\includegraphics[width=1\textwidth]{Image/xxx.pdf}
			\caption[xxx]{Bildunterschrift}
			\label{xxx}
		\end{figure}
		
		
% Beispiel Zitat		
		\begin{quote}
			\textit{Ein Zitat}
		\end{quote}


% Quellenangabe 		\autocite[Seite]{Autor.Jahr}
				\autocite[20]{}

\input{Verzeichnis}
\newpage

\input{Anhang}
\bibdata{Lit}



\end{document}
